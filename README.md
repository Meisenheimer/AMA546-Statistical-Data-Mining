# Statistical-Data-Mining-Project

**If there is any problem, please email me at [zeyu-asparagine.wang@connect.polyu.hk](mailto:zeyu-asparagine.wang@connect.polyu.hk) .**

Requirement:

- `Python`: the required packages are listed in `requirements.txt`, which is generated by `pip freeze`. Most are commonly used (scikit-learn, numpy, matplotlib, scipy, tqdm), except nltk. **We noticed that the version of python and packages might get the results with little different (really little that we believe it is caused by the computer arithmetic or the version of the sklearn, e.g. update the sklearn from 1.3.2 to 1.6.1 will cause the different result by decision tree), but it will not change the results shown in the report.**

The followings are description for each folder:

- `./Codes/`: The main codes for the project, except the lemmatization, stemming and stop words filter part;
- `./Data/`: The codes for downloading data, lemmatizing, stemming and stop words filtering;
- `./Report/`: The final report (**Some result in the report is different from the slide cause we  choose some better parameters**);
- `./Result/`: Some results to support our report and slide. The parameters will be set as shown in the report;
- `./Slide/`: The slide for presentation.

## Preprocessing (`./Data/`)

All the origin data and the preprocessed data are located in  `./Data/Data/`, where the data under `./Data/{year}.tok.pre/` is the data after lemmatizing, stemming and stop words filtering.

**There is no need to run the preprocessing again, unless you want to check whether the nltk package always gives the same results.** By running with `python preprocessing.py`, the files in `./Data/{year}.tok/` will be loaded, processed and then it will output the results to `./Data/{year}.tok.pre/`.

## Main part (`./Codes/`)

There is a `run.sh` in this folder, which including the testing shown as report. You can simply run as it or try more test with the command:

```bash
python --model [Model for testing {Lasso, Ridge, DecisionTree}] --alpha [Parameter for Lasso/Ridge] --max_depth [Parameter for DecisionTree] --min_samples_split [Parameter for DecisionTree] --min_samples_leaf [Parameter for DecisionTree] --target_dim [The number of component given by NMF] --start_year [The time interval for input data (included this year)] --end_year [The time interval for input data (included this year)]
```

There are some other parameters like seed, epoch (the time for repeating the experiment), test_size (rate of the testing data) and iter (the maximum iteration for model fitting), which are not needed to adjust unless any problem occurs e.g. the model is not convergent.

The output folder will be in `./Result/` and named as `./Result/{model}_{start_year}_{end_year}_{time}`.

## Result (`./Result/`)

This folder contains some results we get.

The `draw.py` will draw the MAE and MSE for the result, and `words.py` will analysis the common part and difference of the words features given by different models and through different years. These results will be shown on the report.

The codes will load the data from `./Res-1/` and `./Res-3/`, where `1` and `3` denoted the year interval for training data. If you want to check the correctness, you need to delete the results in two folders and move the new result in (the `./Res-1/` for 1-year data and `./Res-3` for 3 years). Or you can simply check the MAE and MSE at the end of `log.txt`, and the Mean MAE/MSE shows the testing error while the Predict: MAE/MSE shows the predict error.

**Noticed that the version of python and packages might get the results with little different (really little that we believe it is caused by the computer arithmetic or the version of the sklearn, e.g. update the sklearn from 1.3.2 to 1.6.1 will cause the different result by decision tree), but it will not change the results shown in the report.**